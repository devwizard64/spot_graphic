#include <ultra64.h>

.set noreorder
.set noat

/* 0x80329450   void guMtxF2L(float mf[4][4], Mtx *m) */
.globl guMtxF2L
guMtxF2L:
/*0x80329450*/  li      $at, 0x47800000
/*0x80329454*/  mtc1    $at, $f0
/*0x80329458*/  move    $v0, $a1
/*0x8032945C*/  addiu   $v1, $a1, 0x0020
/*0x80329460*/  move    $a2, $0
/*0x80329464*/  move    $a3, $a0
/*0x80329468*/  li      $t4, 0x0004
/*0x8032946C*/  li      $t3, 0x0002
/*0x80329470*/  li      $t2, 0xFFFF0000
.L80329474:
/*0x80329474*/  move    $a0, $0
/*0x80329478*/  move    $t0, $a3
/*0x8032947C*/  lwc1    $f14, 0x0004($t0)
/*0x80329480*/  addiu   $a0, $a0, 0x0001
/*0x80329484*/  lwc1    $f18, 0x0000($t0)
/*0x80329488*/  mul.s   $f16, $f14, $f0
/*0x8032948C*/  beq     $a0, $t3, .L803294F4
/*0x80329490*/  nop
.L80329494:
/*0x80329494*/  mul.s   $f14, $f18, $f0
/*0x80329498*/  addiu   $a0, $a0, 0x0001
/*0x8032949C*/  addiu   $v0, $v0, 0x0004
/*0x803294A0*/  addiu   $v1, $v1, 0x0004
/*0x803294A4*/  addiu   $t0, $t0, 0x0008
/*0x803294A8*/  trunc.w.s $f12, $f16
/*0x803294AC*/  trunc.w.s $f14, $f14
/*0x803294B0*/  mfc1    $t1, $f12
/*0x803294B4*/  mfc1    $a1, $f14
/*0x803294B8*/  sra     $t9, $t1, 16
/*0x803294BC*/  andi    $t5, $t9, 0xFFFF
/*0x803294C0*/  and     $t8, $a1, $t2
/*0x803294C4*/  or      $t6, $t8, $t5
/*0x803294C8*/  sll     $t7, $a1, 16
/*0x803294CC*/  and     $t9, $t7, $t2
/*0x803294D0*/  sw      $t6, -0x0004($v0)
/*0x803294D4*/  andi    $t8, $t1, 0xFFFF
/*0x803294D8*/  or      $t5, $t9, $t8
/*0x803294DC*/  sw      $t5, -0x0004($v1)
/*0x803294E0*/  lwc1    $f14, 0x0004($t0)
/*0x803294E4*/  lwc1    $f18, 0x0000($t0)
/*0x803294E8*/  mul.s   $f16, $f14, $f0
/*0x803294EC*/  bne     $a0, $t3, .L80329494
/*0x803294F0*/  nop
.L803294F4:
/*0x803294F4*/  mul.s   $f14, $f18, $f0
/*0x803294F8*/  addiu   $t0, $t0, 0x0008
/*0x803294FC*/  addiu   $v0, $v0, 0x0004
/*0x80329500*/  addiu   $v1, $v1, 0x0004
/*0x80329504*/  trunc.w.s $f12, $f16
/*0x80329508*/  trunc.w.s $f14, $f14
/*0x8032950C*/  mfc1    $t1, $f12
/*0x80329510*/  mfc1    $a1, $f14
/*0x80329514*/  sra     $t9, $t1, 16
/*0x80329518*/  andi    $t5, $t9, 0xFFFF
/*0x8032951C*/  and     $t8, $a1, $t2
/*0x80329520*/  or      $t6, $t8, $t5
/*0x80329524*/  sll     $t7, $a1, 16
/*0x80329528*/  and     $t9, $t7, $t2
/*0x8032952C*/  andi    $t8, $t1, 0xFFFF
/*0x80329530*/  sw      $t6, -0x0004($v0)
/*0x80329534*/  or      $t5, $t9, $t8
/*0x80329538*/  sw      $t5, -0x0004($v1)
/*0x8032953C*/  addiu   $a2, $a2, 0x0001
/*0x80329540*/  bne     $a2, $t4, .L80329474
/*0x80329544*/  addiu   $a3, $a3, 0x0010
/*0x80329548*/  jr      $ra
/*0x8032954C*/  nop

/* 0x80329550   void guMtxIdentF(float mf[4][4]) */
.globl guMtxIdentF
guMtxIdentF:
/*0x80329550*/  li      $at, 0x3F800000
/*0x80329554*/  move    $v1, $a0
/*0x80329558*/  mtc1    $at, $f0
/*0x8032955C*/  mtc1    $0, $f2
/*0x80329560*/  li      $a0, 0x0001
/*0x80329564*/  move    $v0, $0
/*0x80329568*/  li      $a3, 0x0004
/*0x8032956C*/  li      $a2, 0x0003
/*0x80329570*/  li      $a1, 0x0002
.L80329574:
/*0x80329574*/  bnezl   $v0, .L80329588
/*0x80329578*/  swc1    $f2, 0x0000($v1)
/*0x8032957C*/  b       .L80329588
/*0x80329580*/  swc1    $f0, 0x0000($v1)
/*0x80329584*/  swc1    $f2, 0x0000($v1)
.L80329588:
/*0x80329588*/  bnel    $v0, $a0, .L8032959C
/*0x8032958C*/  swc1    $f2, 0x0004($v1)
/*0x80329590*/  b       .L8032959C
/*0x80329594*/  swc1    $f0, 0x0004($v1)
/*0x80329598*/  swc1    $f2, 0x0004($v1)
.L8032959C:
/*0x8032959C*/  bnel    $v0, $a1, .L803295B0
/*0x803295A0*/  swc1    $f2, 0x0008($v1)
/*0x803295A4*/  b       .L803295B0
/*0x803295A8*/  swc1    $f0, 0x0008($v1)
/*0x803295AC*/  swc1    $f2, 0x0008($v1)
.L803295B0:
/*0x803295B0*/  bnel    $v0, $a2, .L803295C4
/*0x803295B4*/  swc1    $f2, 0x000C($v1)
/*0x803295B8*/  b       .L803295C4
/*0x803295BC*/  swc1    $f0, 0x000C($v1)
/*0x803295C0*/  swc1    $f2, 0x000C($v1)
.L803295C4:
/*0x803295C4*/  addiu   $v0, $v0, 0x0001
/*0x803295C8*/  bne     $v0, $a3, .L80329574
/*0x803295CC*/  addiu   $v1, $v1, 0x0010
/*0x803295D0*/  jr      $ra
/*0x803295D4*/  nop

/* 0x803295D8   void guMtxIdent(Mtx *m) */
.globl guMtxIdent
guMtxIdent:
/*0x803295D8*/  addiu   $sp, $sp, -0x0058
/*0x803295DC*/  sw      $ra, 0x0014($sp)
/*0x803295E0*/  sw      $a0, 0x0058($sp)
/*0x803295E4*/  jal     guMtxIdentF
/*0x803295E8*/  addiu   $a0, $sp, 0x0018
/*0x803295EC*/  addiu   $a0, $sp, 0x0018
/*0x803295F0*/  jal     guMtxF2L
/*0x803295F4*/  lw      $a1, 0x0058($sp)
/*0x803295F8*/  lw      $ra, 0x0014($sp)
/*0x803295FC*/  addiu   $sp, $sp, 0x0058
/*0x80329600*/  jr      $ra
/*0x80329604*/  nop

/* 0x80329608   void guMtxL2F(float mf[4][4], Mtx *m) */
.globl guMtxL2F
guMtxL2F:
/*0x80329608*/  li      $at, 0x47800000
/*0x8032960C*/  mtc1    $at, $f0
/*0x80329610*/  addiu   $sp, $sp, -0x0010
/*0x80329614*/  move    $v0, $a1
/*0x80329618*/  addiu   $v1, $a1, 0x0020
/*0x8032961C*/  move    $a2, $0
/*0x80329620*/  move    $t0, $a0
/*0x80329624*/  li      $t4, 0x0004
/*0x80329628*/  li      $t3, 0x0002
/*0x8032962C*/  li      $t2, 0xFFFF0000
.L80329630:
/*0x80329630*/  move    $a0, $0
/*0x80329634*/  move    $t1, $t0
.L80329638:
/*0x80329638*/  lw      $t6, 0x0000($v1)
/*0x8032963C*/  lw      $t9, 0x0000($v0)
/*0x80329640*/  addiu   $a0, $a0, 0x0001
/*0x80329644*/  srl     $t7, $t6, 16
/*0x80329648*/  andi    $t8, $t7, 0xFFFF
/*0x8032964C*/  and     $t5, $t9, $t2
/*0x80329650*/  or      $t6, $t8, $t5
/*0x80329654*/  sw      $t6, 0x0004($sp)
/*0x80329658*/  lw      $t7, 0x0000($v1)
/*0x8032965C*/  lw      $t8, 0x0000($v0)
/*0x80329660*/  lw      $a1, 0x0004($sp)
/*0x80329664*/  andi    $t9, $t7, 0xFFFF
/*0x80329668*/  sll     $t5, $t8, 16
/*0x8032966C*/  mtc1    $a1, $f18
/*0x80329670*/  and     $t6, $t5, $t2
/*0x80329674*/  or      $a3, $t9, $t6
/*0x80329678*/  cvt.s.w $f18, $f18
/*0x8032967C*/  mtc1    $a3, $f16
/*0x80329680*/  sw      $a3, 0x0000($sp)
/*0x80329684*/  addiu   $v0, $v0, 0x0004
/*0x80329688*/  addiu   $v1, $v1, 0x0004
/*0x8032968C*/  cvt.s.w $f16, $f16
/*0x80329690*/  addiu   $t1, $t1, 0x0008
/*0x80329694*/  div.s   $f18, $f18, $f0
/*0x80329698*/  div.s   $f16, $f16, $f0
/*0x8032969C*/  swc1    $f18, -0x0008($t1)
/*0x803296A0*/  bne     $a0, $t3, .L80329638
/*0x803296A4*/  swc1    $f16, -0x0004($t1)
/*0x803296A8*/  addiu   $a2, $a2, 0x0001
/*0x803296AC*/  bne     $a2, $t4, .L80329630
/*0x803296B0*/  addiu   $t0, $t0, 0x0010
/*0x803296B4*/  jr      $ra
/*0x803296B8*/  addiu   $sp, $sp, 0x0010
